#!/usr/bin/env python3
"""
Sound Alchemist Multi-Modal Agent Test Script
è¿è¡Œå¤šæ¨¡æ€æ™ºèƒ½ä½“æµ‹è¯•éŸ³ä¹æ¸¸æˆ
"""
import time
import numpy as np
import sys
import os
from typing import Dict, Any

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(os.path.dirname(current_dir))
sys.path.insert(0, project_root)

# å¯¼å…¥é…ç½®æ–‡ä»¶
from config import Config, initialize_config

# ä¿®å¤å¯¼å…¥è¯­å¥ - ä½¿ç”¨ç»å¯¹å¯¼å…¥
from game.sound_alchemist.sound_alchemist_env import SoundAlchemistEnv, COLOR_ID_MAP
from game.sound_alchemist.multimodal_agent import MultimodalAgent

def print_observation_info(obs: Dict[str, Any]):
    """æ‰“å°è§‚æµ‹ä¿¡æ¯çš„æ‘˜è¦"""
    image = obs["image"]
    audio = obs["audio"]
    state = obs["state"]
    
    # å›¾åƒç»Ÿè®¡
    unique_colors = len(np.unique(image.reshape(-1, image.shape[-1]), axis=0))
    brightness = np.mean(image)
    
    print(f"  Image: {image.shape}, brightness={brightness:.1f}, unique_colors={unique_colors}")
    print(f"  State: score={state[0]:.0f}, lives={state[1]:.0f}, solved={state[2]:.0f}, tick={state[3]:.0f}")

def main():
    # åˆå§‹åŒ–é…ç½®ç³»ç»Ÿ
    if not initialize_config():
        print("é…ç½®åˆå§‹åŒ–å¤±è´¥ï¼Œé€€å‡ºç¨‹åº")
        return
    
    print("=== Sound Alchemist Multi-Modal Agent Test ===")
    
    # ä½¿ç”¨é…ç½®åˆ›å»ºç¯å¢ƒ
    env = SoundAlchemistEnv(
        difficulty=Config.DEFAULT_DIFFICULTY,
        save_data=Config.Environment.SAVE_DATA,
        save_sequence=Config.Environment.SAVE_SEQUENCE,
        save_dir=Config.Environment.SAVE_DIR
    )
    
    # ä½¿ç”¨é…ç½®åˆ›å»ºæ™ºèƒ½ä½“
    agent = MultimodalAgent(
        verbose=Config.Agent.VERBOSE, 
        use_local_fallback=Config.Agent.USE_LOCAL_FALLBACK,
        conversation_strategy=Config.Agent.CONVERSATION_STRATEGY,
        max_retries=Config.Agent.MAX_RETRIES
    )
    
    # è¿æ¥æ™ºèƒ½ä½“ä¸ç¯å¢ƒï¼Œè®©æ™ºèƒ½ä½“èƒ½å¤Ÿè·å–æ¸¸æˆçŠ¶æ€ä¿¡æ¯
    agent.set_game_environment(env)
    
    try:
        # è¿è¡Œå¤šä¸ªepisode
        num_episodes = Config.Experiment.NUM_EPISODES
        
        for episode in range(num_episodes):
            print(f"\n{'='*50}")
            print(f"Episode {episode + 1}/{num_episodes}")
            print(f"{'='*50}")
            
            # é‡ç½®æ™ºèƒ½ä½“çŠ¶æ€ - ä½¿ç”¨ç°æœ‰çš„æ–¹æ³•æˆ–æ‰‹åŠ¨é‡ç½®
            if hasattr(agent, 'reset_for_new_episode'):
                agent.reset_for_new_episode(episode + 1)
            else:
                # æ‰‹åŠ¨é‡ç½®æ™ºèƒ½ä½“çš„episodeç›¸å…³çŠ¶æ€
                agent.current_episode = episode + 1
                agent.current_step = 0
                agent.current_round_start_time = time.time()
                agent.last_game_completed = False
                # é‡ç½®è®°å¿†ç®¡ç†å™¨
                if hasattr(agent.memory_manager, 'reset_for_new_episode'):
                    agent.memory_manager.reset_for_new_episode(episode + 1)
                print(f"Agent manually reset for episode {episode + 1}")
            
            obs, info = env.reset()
            episode_reward = 0
            step_count = 0
            max_steps = Config.Environment.MAX_STEPS_PER_EPISODE
            game_completed = False  
            
            print("Initial observation:")
            print_observation_info(obs)
            
            # æ˜¾ç¤ºå½“å‰æ¸¸æˆä¸­çš„å¯ç”¨é¢œè‰²æ–¹å—
            if hasattr(env, 'get_current_available_colors'):
                available_colors = env.get_current_available_colors()
                print(f"Available color blocks this round: {available_colors}")
            
            while step_count < max_steps and not game_completed:
                print(f"\n--- Step {step_count + 1} ---")
                
                # æ™ºèƒ½ä½“é€‰æ‹©åŠ¨ä½œ
                action, end_game = agent.act(obs)
                if end_game:
                    game_completed = True
                    print("Game ended.")
                    break

                color_name = list(COLOR_ID_MAP.keys())[action]
                print(f"Agent chose action {action} ({color_name})")
                
                # è·å–æ™ºèƒ½ä½“çš„æ–‡æœ¬è¾“å‡º
                current_text = agent.get_current_step_text()
                if current_text:
                    # å¦‚æœç¯å¢ƒæ”¯æŒè®¾ç½®æ­¥éª¤æ–‡æœ¬ï¼Œåˆ™è®¾ç½®
                    if hasattr(env, 'set_step_text'):
                        env.set_step_text(current_text)
                    print(f"Agent reasoning: {current_text[:100]}..." if len(current_text) > 100 else f"Agent reasoning: {current_text}")
                
                # æ˜¾ç¤ºæœ€è¿‘çš„æ¨¡å‹è¾“å‡ºä¿¡æ¯å’Œæ€è€ƒè¿‡ç¨‹
                if agent.model_output_history:
                    latest_output = agent.model_output_history[-1]
                    if latest_output.get("success"):
                        response_time = latest_output.get("response_time", 0)
                        response_text = latest_output.get("response_text", "")
                        print(f"Model response time: {response_time:.2f}s, length: {len(response_text)} chars")
                        
                        # æ˜¾ç¤ºæ€è€ƒè¿‡ç¨‹çš„å…³é”®éƒ¨åˆ†
                        if response_text and len(response_text) > 50:
                            # å°è¯•æå–å†³ç­–éƒ¨åˆ†
                            decision_start = response_text.upper().find("DECISION:")
                            if decision_start >= 0:
                                decision_text = response_text[decision_start:decision_start+200]
                                print(f"Model reasoning: {decision_text}...")
                            else:
                                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°DECISIONæ ‡ç­¾ï¼Œæ˜¾ç¤ºæœ€å100å­—ç¬¦
                                print(f"Model thinking: ...{response_text[-100:]}")

                # æ˜¾ç¤ºå½“å‰è®°å¿†çŠ¶æ€
                memory_stats = agent.memory_manager.get_conversation_context_for_api("hybrid")
                if step_count > 0:
                    print(f"Memory context length: {len(memory_stats)} chars")

                try:
                    # æ‰§è¡ŒåŠ¨ä½œ
                    obs, reward, terminated, truncated, info = env.step(action)
                    episode_reward += reward
                    step_count += 1
                    
                    print(f"Reward: {reward}, Total: {episode_reward}")
                    
                    # æ£€æŸ¥æ¸¸æˆé€šå…³çŠ¶æ€
                    current_solved = float(obs["state"][2])
                    current_score = float(obs["state"][0])
                    
                    # å¤šç§æ–¹å¼æ£€æµ‹æ¸¸æˆé€šå…³
                    if current_solved > 0:
                        game_completed = True
                        print("ğŸ‰ GAME COMPLETED! Sequence successfully matched!")
                    elif reward >= Config.Scoring.COMPLETION_MULTIPLIER:
                        game_completed = True
                        print("ğŸ‰ GAME COMPLETED! High reward achieved!")
                    elif "game_completed" in info and info["game_completed"]:
                        game_completed = True
                        print("ğŸ‰ GAME COMPLETED! Environment confirmed completion!")
                    
                    # å¦‚æœæ¸¸æˆé€šå…³ï¼Œè¿›è¡Œåˆ†æ•°ç»“ç®—
                    if game_completed:
                        print(f"\n{'='*40}")
                        print(f"ğŸ† ROUND COMPLETION SUMMARY ğŸ†")
                        print(f"{'='*40}")
                        print(f"Final Score: {current_score}")
                        print(f"Total Steps: {step_count}")
                        print(f"Episode Reward: {episode_reward}")
                        print(f"Efficiency: {current_score/step_count:.2f} points/step")
                        
                        # è·å–æ™ºèƒ½ä½“é€šå…³ç»Ÿè®¡ - ä½¿ç”¨å®‰å…¨çš„æ–¹å¼
                        if hasattr(agent, 'get_completion_stats'):
                            completion_stats = agent.get_completion_stats()
                            print(f"Agent Completions: {completion_stats['total_completions']}")
                            if completion_stats['total_completions'] > 0:
                                print(f"Average Completion Score: {completion_stats['average_score']:.1f}")
                                print(f"Best Score: {completion_stats['best_score']}")
                                print(f"Completion Rate: {completion_stats['completion_rate']:.1f}%")
                        
                        # è·å–å­¦ä¹ è¿›åº¦æ‘˜è¦ - ä½¿ç”¨å®‰å…¨çš„æ–¹å¼
                        if hasattr(agent, 'get_learning_progress_summary'):
                            learning_summary = agent.get_learning_progress_summary()
                            print(f"\n{learning_summary}")
                        
                        # æä¾›æ­£é¢åé¦ˆç»™æ™ºèƒ½ä½“
                        agent.update_color_feedback(color_name, True)
                        
                        # æ˜¾ç¤ºåºåˆ—å®Œæˆä¿¡æ¯
                        if hasattr(env, '_build_detailed_game_state'):
                            try:
                                detailed_state = env._build_detailed_game_state(action=None, is_reset=False)
                                completed_sequence = detailed_state.get("current_correct_sequence", [])
                                if completed_sequence:
                                    print(f"Completed Sequence: {' â†’ '.join(completed_sequence)}")
                            except Exception as e:
                                print(f"Could not get sequence details: {e}")
                        
                        # æ‰‹åŠ¨è®°å½•é€šå…³ä¿¡æ¯ï¼ˆå¦‚æœæ™ºèƒ½ä½“æ²¡æœ‰è‡ªåŠ¨å¤„ç†ï¼‰
                        if not hasattr(agent, 'game_completion_history'):
                            agent.game_completion_history = []
                        
                        completion_record = {
                            "timestamp": time.time(),
                            "episode": episode + 1,
                            "final_score": current_score,
                            "steps_taken": step_count,
                            "episode_reward": episode_reward,
                            "efficiency": current_score / step_count if step_count > 0 else 0
                        }
                        agent.game_completion_history.append(completion_record)
                        
                        break
                    
                    # æ˜¾ç¤ºæ¸¸æˆçŠ¶æ€ä¸­çš„é‡è¦ä¿¡æ¯
                    if "needs_restart_from_beginning" in info:
                        print(f"Needs restart: {info['needs_restart_from_beginning']}")
                    if "sequence_reset" in info:
                        print(f"Sequence reset: {info['sequence_reset']}")
                    
                    if reward > 0 and not game_completed:
                        print("âœ“ Positive reward achieved!")
                        # æä¾›é¢œè‰²åé¦ˆç»™æ™ºèƒ½ä½“
                        agent.update_color_feedback(color_name, True)
                    elif reward <= 0:
                        agent.update_color_feedback(color_name, False)
                    
                    print_observation_info(obs)
                    
                    # æ¯5æ­¥æ˜¾ç¤ºä¸€æ¬¡è®°å¿†ç»Ÿè®¡
                    if step_count % 5 == 0:
                        rounds_count = len(agent.memory_manager.conversation_rounds) if hasattr(agent, 'memory_manager') else 0
                        summaries_count = len(agent.memory_manager.compressed_summaries) if hasattr(agent, 'memory_manager') and hasattr(agent.memory_manager, 'compressed_summaries') else 0
                        
                        # å®‰å…¨åœ°è·å–æ¨¡å‹ç»Ÿè®¡
                        if hasattr(agent, 'get_model_output_stats'):
                            model_stats = agent.get_model_output_stats()
                            print(f"ğŸ“Š Memory: {rounds_count} active rounds, {summaries_count} compressed summaries")
                            print(f"ğŸ“ˆ Model: {model_stats['successful_calls']}/{model_stats['total_calls']} calls, "
                                  f"avg response time: {model_stats.get('avg_response_time', 0):.2f}s")
                        else:
                            print(f"ğŸ“Š Memory: {rounds_count} active rounds, {summaries_count} compressed summaries")
                            print(f"ğŸ“ˆ Model: Statistics not available")
                    
                    # æ£€æŸ¥ç¯å¢ƒæ˜¯å¦è‡ªç„¶ç»“æŸ
                    if terminated or truncated:
                        print(f"Episode ended: terminated={terminated}, truncated={truncated}")
                        
                        # å¦‚æœæ²¡æœ‰é€šå…³ä½†episodeç»“æŸï¼Œæ£€æŸ¥æ˜¯å¦æ˜¯å¤±è´¥
                        if not game_completed:
                            lives_remaining = float(obs["state"][1])
                            if lives_remaining <= 0:
                                print("ğŸ’€ Game Over - No lives remaining")
                            else:
                                print("â° Episode ended - Time/step limit reached")
                        
                        break
                        
                except Exception as e:
                    print(f"Error during step {step_count}: {e}")
                    import traceback
                    traceback.print_exc()
                    break
        
            # æ˜¾ç¤ºå­¦ä¹ åˆ°çš„æ˜ å°„
            if agent.learned_color_note_mapping:
                print(f"  Learned mappings: {agent.learned_color_note_mapping}")
            
            time.sleep(1)  # çŸ­æš‚æš‚åœ
        
        # æ‰€æœ‰episodeç»“æŸåçš„æœ€ç»ˆæ€»ç»“
        print(f"\n{'='*60}")
        print("ğŸ FINAL GAME COMPLETION SUMMARY")
        print(f"{'='*60}")
        
        # æ˜¾ç¤ºæ¸¸æˆè¯„åˆ†æœºåˆ¶
        if hasattr(env, 'get_current_game_scoring_mechanism'):
            scoring_mechanism = env.get_current_game_scoring_mechanism()
            print(f"\nğŸ“‹ Current Game Scoring Mechanism:")
            print(f"{'='*50}")
            
            # åŸå§‹æ¸¸æˆè¯„åˆ†
            game_scoring = scoring_mechanism["game_scoring"]
            print(f"ğŸ® Original Game Scoring:")
            print(f"  Base Score: {game_scoring['base_score']}")
            print(f"  Difficulty Multipliers: {game_scoring['difficulty_multipliers']}")
            print(f"  Perfect Play Bonus: {game_scoring['bonuses']['perfect_play']}")
            print(f"  Sequence Bonus: {game_scoring['bonuses']['sequence_bonus']}")
            print(f"  Mistake Penalty: {game_scoring['penalties']['mistake_penalty']}")
        
    except KeyboardInterrupt:
        print("\nInterrupted by user")
    except Exception as e:
        print(f"Unexpected error: {e}")
        import traceback
        traceback.print_exc()
    finally:
        # ç¡®ä¿ä¿å­˜æ•°æ®
        try:
            env.close()
        except:
            pass

if __name__ == "__main__":
    main()